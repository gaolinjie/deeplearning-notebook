{"compress":true,"commitItems":[["2f95e1a7-95c3-4c2f-810e-ee8b6e64c2ed",1525490642861,"",[[1525490626781,["gao@gggg",[[1,0,"（5）-- 深层神经网络\n===\n\n\n"]],[0,0],[19,19]]],[1525490640538,["gao@gggg",[[1,0,"> 我的CSDN博客地址：[红色石头的专栏](https://link.zhihu.com/?target=http%3A//blog.csdn.net/red_stone1)\n> 我的知乎主页：[红色石头](https://www.zhihu.com/people/red_stone_wl)\n> 我的微博：[RedstoneWill的微博](https://link.zhihu.com/?target=https%3A//weibo.com/6479023696/profile%3Ftopnav%3D1%26wvr%3D6%26is_all%3D1)\n> 我的GitHub：[RedstoneWill的GitHub](https://link.zhihu.com/?target=https%3A//github.com/RedstoneWill)\n> 我的微信公众号：红色石头的机器学习之路"],[1,1,"ID：redstonewill）\n> 欢迎大家关注我！共同学习，共同进步！\n\n上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延伸和扩展，讨论更深层的神经网络。\n\n## **1\\. Deep L-layer neural network**\n\n深层神经网络其实就是包含更多的隐藏层神经网络。如下图所示，分别列举了逻辑回归、1个隐藏层的神经网络、2个隐藏层的神经网络和"],[1,2,"个隐藏层的神经网络它们的模型结构。\n\n![](https://pic3.zhimg.com/80/v2-3d6d8d46821398f6f38b32fff994d0aa_hd.jpg)\n\n命名规则上，一般只参考隐藏层个数和输出层。例如，上图中的逻辑回归又叫1 layer NN，1个隐藏层的神经网络叫做2 layer NN，2个隐藏层的神经网络叫做3 layer NN，以此类推。如果是L-layer NN，则包含了L-1个隐藏层，最后的L层是输出层。\n\n下面以一个4层神经网络为例来介绍关于神经网络的一些标记写法。如下图所示，首先，总层数用L表示，L=4。输入层是第0层，输出层是第L层。 ![n^{[l]}](https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D) 表示第 ![l ](https://www.zhihu.com/equation?tex=l+) 层包含的单元个数， ![l=0,1,\\cdots,L](https://www.zhihu.com/equation?tex=l%3D0%2C1%2C%5Ccdots%2CL) 。这个模型中， ![n^{[0]}=n_x=3](https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D%3Dn_x%3D3) ，表示三个输入特征 ![x_1,x_2,x_3](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3) 。 ![n^{[1]}=5](https://www.zhihu.com/equation?tex=n%5E%7B%5B1%5D%7D%3D5) ， ![n^{[2]}=5](https://www.zhihu.com/equation?tex=n%5E%7B%5B2%5D%7D%3D5) ， ![n^{[3]}=3](https://www.zhihu.com/equation?tex=n%5E%7B%5B3%5D%7D%3D3) ， ![n^{[4]}=n^{[L]}=1](https://www.zhihu.com/equation?tex=n%5E%7B%5B4%5D%7D%3Dn%5E%7B%5BL%5D%7D%3D1) 。第 ![l](https://www.zhihu.com/equation?tex=l) 层的激活函数输出用 ![a^{[l]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D) 表示， ![a^{[l]}=g^{[l]}(z^{[l]})](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28z%5E%7B%5Bl%5D%7D%29) 。 ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 表示第 ![l](https://www.zhihu.com/equation?tex=l) 层的权重，用于计算 ![z^{[l]}](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D) 。另外，我们把输入x记为 ![a^{[0]}](https://www.zhihu.com/equation?tex=a%5E%7B%5B0%5D%7D) ，把输出层 ![\\hat y](https://www.zhihu.com/equation?tex=%5Chat+y) 记为 ![a^{[L]}](https://www.zhihu.com/equation?tex=a%5E%7B%5BL%5D%7D) 。\n\n注意， ![a^{[l]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D) 和 ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 中的上标 ![l](https://www.zhihu.com/equation?tex=l) 都是从1开始的， ![l=1,\\cdots,L](https://www.zhihu.com/equation?tex=l%3D1%2C%5Ccdots%2CL) 。\n\n![](https://pic2.zhimg.com/80/v2-f766e253c42fc19cc9888029165762e1_hd.jpg)\n\n## **2\\. Forward Propagation in a Deep Network**\n\n接下来，我们来推导一下深层神经网络的正向传播过程。仍以上面讲过的4层神经网络为例，对于单个样本：\n\n第1层， ![l=1](https://www.zhihu.com/equation?tex=l%3D1) ：\n\n![z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}](https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D%3DW%5E%7B%5B1%5D%7Dx%2Bb%5E%7B%5B1%5D%7D%3DW%5E%7B%5B1%5D%7Da%5E%7B%5B0%5D%7D%2Bb%5E%7B%5B1%5D%7D)\n\n![a^{[1]}=g^{[1]}(z^{[1]})](https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D%3Dg%5E%7B%5B1%5D%7D%28z%5E%7B%5B1%5D%7D%29)\n\n第2层， ![l=2](https://www.zhihu.com/equation?tex=l%3D2) ：\n\n![z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}](https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D%7D%3DW%5E%7B%5B2%5D%7Da%5E%7B%5B1%5D%7D%2Bb%5E%7B%5B2%5D%7D)\n\n![a^{[2]}=g^{[2]}(z^{[2]})](https://www.zhihu.com/equation?tex=a%5E%7B%5B2%5D%7D%3Dg%5E%7B%5B2%5D%7D%28z%5E%7B%5B2%5D%7D%29)\n\n第3层， ![l=3](https://www.zhihu.com/equation?tex=l%3D3) ：\n\n![z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}](https://www.zhihu.com/equation?tex=z%5E%7B%5B3%5D%7D%3DW%5E%7B%5B3%5D%7Da%5E%7B%5B2%5D%7D%2Bb%5E%7B%5B3%5D%7D)\n\n![a^{[3]}=g^{[3]}(z^{[3]})](https://www.zhihu.com/equation?tex=a%5E%7B%5B3%5D%7D%3Dg%5E%7B%5B3%5D%7D%28z%5E%7B%5B3%5D%7D%29)\n\n第4层， ![l=4](https://www.zhihu.com/equation?tex=l%3D4) ：\n\n![z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}](https://www.zhihu.com/equation?tex=z%5E%7B%5B4%5D%7D%3DW%5E%7B%5B4%5D%7Da%5E%7B%5B3%5D%7D%2Bb%5E%7B%5B4%5D%7D)\n\n![a^{[4]}=g^{[4]}(z^{[4]})](https://www.zhihu.com/equation?tex=a%5E%7B%5B4%5D%7D%3Dg%5E%7B%5B4%5D%7D%28z%5E%7B%5B4%5D%7D%29)\n\n如果有m个训练样本，其向量化矩阵形式为：\n\n第1层，l ![l=1](https://www.zhihu.com/equation?tex=l%3D1) ：\n\n![Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5B1%5D%7D%3DW%5E%7B%5B1%5D%7DX%2Bb%5E%7B%5B1%5D%7D%3DW%5E%7B%5B1%5D%7DA%5E%7B%5B0%5D%7D%2Bb%5E%7B%5B1%5D%7D)\n\n![A^{[1]}=g^{[1]}(Z^{[1]})](https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D%3Dg%5E%7B%5B1%5D%7D%28Z%5E%7B%5B1%5D%7D%29)\n\n第2层， ![l=2](https://www.zhihu.com/equation?tex=l%3D2) ：\n\n![Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5B2%5D%7D%3DW%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D%2Bb%5E%7B%5B2%5D%7D)\n\n![A^{[2]}=g^{[2]}(Z^{[2]})](https://www.zhihu.com/equation?tex=A%5E%7B%5B2%5D%7D%3Dg%5E%7B%5B2%5D%7D%28Z%5E%7B%5B2%5D%7D%29)\n\n第3层， ![l=3](https://www.zhihu.com/equation?tex=l%3D3) ：\n\n![Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5B3%5D%7D%3DW%5E%7B%5B3%5D%7DA%5E%7B%5B2%5D%7D%2Bb%5E%7B%5B3%5D%7D)\n\n![A^{[3]}=g^{[3]}(Z^{[3]})](https://www.zhihu.com/equation?tex=A%5E%7B%5B3%5D%7D%3Dg%5E%7B%5B3%5D%7D%28Z%5E%7B%5B3%5D%7D%29)\n\n第4层， ![l=4](https://www.zhihu.com/equation?tex=l%3D4) ：\n\n![Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5B4%5D%7D%3DW%5E%7B%5B4%5D%7DA%5E%7B%5B3%5D%7D%2Bb%5E%7B%5B4%5D%7D)\n\n![A^{[4]}=g^{[4]}(Z^{[4]})](https://www.zhihu.com/equation?tex=A%5E%7B%5B4%5D%7D%3Dg%5E%7B%5B4%5D%7D%28Z%5E%7B%5B4%5D%7D%29)\n\n综上所述，对于第 ![l](https://www.zhihu.com/equation?tex=l) 层，其正向传播过程的 ![Z^{[l]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D) 和 ![A^{[l]}](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D) 可以表示为：\n\n![Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D)\n\n![A^{[l]}=g^{[l]}(Z^{[l]})](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29)\n\n其中 ![l=1,\\cdots,L](https://www.zhihu.com/equation?tex=l%3D1%2C%5Ccdots%2CL)\n\n## **3\\. Getting your matrix dimensions right**\n\n对于单个训练样本，输入x的维度是（ ![n^{[0]},1](https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D%2C1) "],[1,3,"神经网络的参数 ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 和 ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 的维度分别是：\n\n![W^{[l]}:\\ (n^{[l]},n^{[l"],[1,4,"1]})](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7B%5Bl"],[1,5,"1%5D%7D%29)\n\n![b^{[l]}:\\"],[1,6,"(n^{[l]},1)](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2C1%29)\n\n其中， ![l=1,\\cdots,L](https://www.zhihu.com/equation?tex=l%3D1%2C%5Ccdots%2CL) ， ![n^{[l]}](https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D) 和 ![n^{[l-1]}](https://www.zhihu.com/equation?tex=n%5E%7B%5Bl-1%5D%7D) 分别表示第 ![l](https://www.zhihu.com/equation?tex=l) 层和 ![l-1](https://www.zhihu.com/equation?tex=l-1) 层的所含单元个数。 ![n^{[0]}=n_x](https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D%3Dn_x) ，表示输入层特征数目。\n\n顺便提一下，反向传播过程中的 ![dW^{[l]}](https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D) 和 ![db^{[l]}](https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D) 的维度分别是：\n\n![dW^{[l]}:\\ (n^{[l]},n^{[l-1]})](https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7B%5Bl-1%5D%7D%29)\n\n![db^{[l]}:\\ (n^{[l]},1)](https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2C1%29)\n\n注意到， ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 与 ![dW^{[l]}](https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D) 维度相同， ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 与 ![db^{[l]}](https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D) 维度相同。这很容易理解。\n\n正向传播过程中的 ![z^{[l]}](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D) 和 ![a^{[l]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D) 的维度分别是：\n\n![z^{[l]}:\\ (n^{[l]},1)](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2C1%29)\n\n![a^{[l]}:\\ (n^{[l]},1)](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2C1%29)\n\n![z^{[l]}](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D) 和 ![a^{[l]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D) 的维度是一样的，且 ![dz^{[l]}](https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D) 和 ![da^{[l]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D) 的维度均与 ![z^{[l]}](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D) 和 ![a^{[l]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D) 的维度一致。\n\n对于m个训练样本，输入矩阵X的维度是（ ![n^{[0]},m](https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D%2Cm) ）。需要注意的是 ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 和 ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 的维度与只有单个样本是一致的：\n\n![W^{[l]}:\\ (n^{[l]},n^{[l-1]})](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7B%5Bl-1%5D%7D%29)\n\n![b^{[l]}:\\ (n^{[l]},1)](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2C1%29)\n\n只不过在运算 ![Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D) 中， ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 会被当成（ ![n^{[l]},m](https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D%2Cm) ）矩阵进行运算，这是因为python的广播性质，且 ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 每一列向量都是一样的。 ![dW^{[l]}](https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D) 和 ![db^{[l]}](https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D) 的维度分别与 ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 和 ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 的相同。\n\n但是， ![Z^{[l]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D) 和 ![A^{[l]}](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D) 的维度发生了变化：\n\n![Z^{[l]}:\\ (n^{[l]},m)](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2Cm%29)\n\n![A^{[l]}:\\ (n^{[l]},m)](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D%3A%5C+%28n%5E%7B%5Bl%5D%7D%2Cm%29)\n\n![dZ^{[l]}](https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D) 和 ![dA^{[l]}](https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl%5D%7D) 的维度分别与 ![Z^{[l]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D) 和 ![A^{[l]}](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D) 的相同。\n\n## **4\\. Why deep representations?**\n\n我们都知道神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够“"],[1,7,"”，也就是说网络"],[1,8,"数越多，"],[1,12,"就更加复杂和深入，学习也更加准确。接下来，我们从几个例子入手，看一下为什么深度网络能够如此强大。"],[1,13,"\n先来看人脸识别的例子，如下图所示。经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。可以看出，随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。\n\n语音识别模型也是这个道理。浅层的神经元能够检测一些简单的音调，然后较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。记住一点，神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。\n\n![](https://pic1.zhimg.com/80/v2-2c245aac94020ea2bf466faaab40e156_hd.jpg)\n\n除了从提取特征复杂度的角度来说明深层网络的优势之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量。例如下面这个例子，使用电路理论，计算逻辑输出：\n\n![y"],[1,14,"x_1\\oplus x_2\\oplus x_3\\oplus\\cdots\\oplus x_n](https://www.zhihu.com/equation?tex"],[1,15,"y%3Dx_1%5Coplus+x_2%5Coplus+x_3%5Coplus%5Ccdots%5Coplus+x_n)\n\n其中， ![\\oplus](https://www.zhihu.com/equation?tex"],[1,16,"%5Coplus) 表示异或操作。对于这个逻辑运算，如果使用深度网络，深度网络的结构是每层将前一层的两两单元进行异或，最后到一个输出，如下图左边所示。这样，整个深度网络的层数是 ![log_2(n)](https://www.zhihu.com/equation?tex=log_2%28n%29) ，不包含输入层。总共使用的神经元个数为："],[1,18,"![1+2+\\cdots+2^{log_2(n)-1}=1\\cdot\\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1](https://www.zhihu.com/equation?tex=1%2B2%2B%5Ccdots%2B2%5E%7Blog_2%28n%29-1%7D%3D1%5Ccdot%5Cfrac%7B1-2%5E%7Blog_2%28n%29%7D%7D%7B1-2%7D%3D2%5E%7Blog_2%28n%29%7D-1%3Dn-1)"],[1,19,"\n可见，输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个。\n\n如果不用深层网络，仅仅使用单个隐藏层，那么需要的神经元个数将是指数级别那么大。Andrew指出，由于包含了所有的逻辑位（0和1），则需要 ![2^{n-1}](https://www.zhihu.com/equation?tex=2%5E%7Bn-1%7D) 个神经元。**这里笔者推导的是** ![2^n](https://www.zhihu.com/equation?tex=2%5En) **个神经元，为啥是** ![2^{n-1}](https://www.zhihu.com/equation?tex=2%5E%7Bn-1%7D) **请哪位高手解释下。**\n\n比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。\n\n尽管深度学习有着非常显著的优势，Andrew还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律（Occam’s Razor）。对于比较复杂的问题，再使用较深的神经网络模型。\n\n## **5\\. Building blocks of deep neural networks**\n\n下面用流程块图来解释神经网络正向传播和反向传播过程。如下图所示，对于第l层来说，正向传播过程中：\n\n输入： ![a^{[l-1]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D)\n\n输出： ![a^{[l]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D)\n\n参数： ![W^{[l]},b^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D)\n\n缓存变量： ![z^{[l]}](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D)\n\n反向传播过程中：\n\n输入： ![da^{[l]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D)\n\n输出： ![da^{[l-1]},dW^{[l]},db^{[l]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D%2CdW%5E%7B%5Bl%5D%7D%2Cdb%5E%7B%5Bl%5D%7D)\n\n参数： ![W^{[l]},b^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D)\n\n![](https://pic2.zhimg.com/80/v2-dad16d5de3c10166df1cc0c112f3dfb2_hd.jpg)\n\n刚才这是第 ![l](https://www.zhihu.com/equation?tex=l) 层的流程块图，对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：\n\n![](https://pic4.zhimg.com/80/v2-45cadf1acbaf009556f5aee163f5f85c_hd.jpg)\n\n## **6\\. Forward and Backward Propagation**\n\n我们继续接着上一部分流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。\n\n首先是正向传播过程，令层数为第 ![l](https://www.zhihu.com/equation?tex=l) 层，输入是 ![a^{[l-1]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D) ，输出是 ![a^{[l]}](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D) ，缓存变量是 ![z^{[l]}](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D) 。其表达式如下：\n\n![z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}](https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D)\n\n![a^{[l]}=g^{[l]}(z^{[l]})](https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28z%5E%7B%5Bl%5D%7D%29)\n\nm个训练样本，向量化形式为：\n\n![Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D)\n\n![A^{[l]}=g^{[l]}(Z^{[l]})](https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29)\n\n然后是反向传播过程，输入是 ![da^{[l]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D) ，输出是 ![da^{[l-1]},dw^{[l]},db^{[l]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D%2Cdw%5E%7B%5Bl%5D%7D%2Cdb%5E%7B%5Bl%5D%7D) 。其表达式如下：\n\n![dz^{[l]}=da^{[l]}\\ast g^{[l]'}(z^{[l]})](https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D%3Dda%5E%7B%5Bl%5D%7D%5Cast+g%5E%7B%5Bl%5D%27%7D%28z%5E%7B%5Bl%5D%7D%29)\n\n![dW^{[l]}=dz^{[l]}\\cdot a^{[l-1]}](https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D%3Ddz%5E%7B%5Bl%5D%7D%5Ccdot+a%5E%7B%5Bl-1%5D%7D)\n\n![db^{[l]}=dz^{[l]}](https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D%3Ddz%5E%7B%5Bl%5D%7D)\n\n![da^{[l-1]}=W^{[l]T}\\cdot dz^{[l]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D%3DW%5E%7B%5Bl%5DT%7D%5Ccdot+dz%5E%7B%5Bl%5D%7D)\n\n由上述第四个表达式可得 ![da^{[l]}=W^{[l+1]T}\\cdot dz^{[l+1]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%2B1%5DT%7D%5Ccdot+dz%5E%7B%5Bl%2B1%5D%7D) ，将 ![da^{[l]}](https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D) 代入第一个表达式中可以得到：\n\n![dz^{[l]}=W^{[l+1]T}\\cdot dz^{[l+1]}\\ast g^{[l]'}(z^{[l]})](https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%2B1%5DT%7D%5Ccdot+dz%5E%7B%5Bl%2B1%5D%7D%5Cast+g%5E%7B%5Bl%5D%27%7D%28z%5E%7B%5Bl%5D%7D%29)\n\n该式非常重要，反映了 ![dz^{[l+1]}](https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%2B1%5D%7D) 与 ![dz^{[l]}](https://www.zhihu.com/equation?tex=dz%5E%7B%5Bl%5D%7D) 的递推关系。\n\nm个训练样本，向量化形式为：\n\n![dZ^{[l]}=dA^{[l]}\\ast g^{[l]'}(Z^{[l]})](https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D%3DdA%5E%7B%5Bl%5D%7D%5Cast+g%5E%7B%5Bl%5D%27%7D%28Z%5E%7B%5Bl%5D%7D%29)\n\n![dW^{[l]}=\\frac1mdZ^{[l]}\\cdot A^{[l-1]T}](https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D%3D%5Cfrac1mdZ%5E%7B%5Bl%5D%7D%5Ccdot+A%5E%7B%5Bl-1%5DT%7D)\n\n![db^{[l]}=\\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True)](https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D%3D%5Cfrac1mnp.sum%28dZ%5E%7B%5Bl%5D%7D%2Caxis%3D1%2Ckeepdim%3DTrue%29)\n\n![dA^{[l-1]}=W^{[l]T}\\cdot dZ^{[l]}](https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl-1%5D%7D%3DW%5E%7B%5Bl%5DT%7D%5Ccdot+dZ%5E%7B%5Bl%5D%7D)\n\n![dZ^{[l]}=W^{[l+1]T}\\cdot dZ^{[l+1]}\\ast g^{[l]'}(Z^{[l]})](https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%2B1%5DT%7D%5Ccdot+dZ%5E%7B%5Bl%2B1%5D%7D%5Cast+g%5E%7B%5Bl%5D%27%7D%28Z%5E%7B%5Bl%5D%7D%29)\n\n## **7\\. Parameters vs Hyperparameters**\n\n该部分介绍神经网络中的参数（parameters）和超参数（hyperparameters）的概念。\n\n神经网络中的参数就是我们熟悉的 ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 和 ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 。而超参数则是例如学习速率 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) ，训练迭代次数N，神经网络层数L，各层神经元个数 ![n^{[l]}](https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D) ，激活函数 ![g(z)](https://www.zhihu.com/equation?tex=g%28z%29) 等。之所以叫做超参数的原因是它们决定了参数 ![W^{[l]}](https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D) 和 ![b^{[l]}](https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D) 的值。在后面的第二门课我们还将学习其它的超参数，这里先不讨论。\n\n如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。\n\n## **8\\. What does this have to do with the brain?**\n\n那么，神经网络跟人脑机制到底有什么联系呢？究竟有多少的相似程度？神经网络实际上可以分成两个部分：正向传播过程和反向传播过程。神经网络的每个神经元采用激活函数的方式，类似于感知机模型。这种模型与人脑神经元是类似的，可以说是一种非常简化的人脑神经元模型。如下图所示，人脑神经元可分为树突、细胞体、轴突三部分。树突接收外界电刺激信号（类比神经网络中神经元输入），传递给细胞体进行处理（类比神经网络中神经元激活函数运算），最后由轴突传递给下一个神经元（类比神经网络中神经元输出）。\n\n值得一提的是，人脑神经元的结构和处理方式要复杂的多，神经网络模型只是非常简化的模型。人脑如何进行学习？是否也是通过反向传播和梯度下降算法现在还不清楚，可能会更加复杂。这是值得生物学家探索的事情。也许发现重要的新的人脑学习机制后，让我们的神经网络模型抛弃反向传播和梯度下降算法，能够实现更加准确和强大的神经网络模型！\n\n![](https://pic1.zhimg.com/80/v2-a53dd96f03fd28ade6a51043f5679b44_hd.jpg)\n\n## **9\\. Summary**\n\n本节课主要介绍了深层神经网络，是上一节浅层神经网络的拓展和归纳。首先，我们介绍了建立神经网络模型一些常用的标准的标记符号。然后，用流程块图的方式详细推导正向传播过程和反向传播过程的输入输出和参数表达式。我们也从提取特征复杂性和计算量的角度分别解释了深层神经网络为什么优于浅层神经网络。接着，我们介绍了超参数的概念，解释了超参数与参数的区别。最后，我们将神经网络与人脑做了类别，人工神经网络是简化的人脑模型。\n\n**至此，Andew深度学习专项课程第一门课《神经网络与深度学习》结束。如果觉得还行的话，请点赞支持下。**\n\n**更多AI资源请关注公众号：红色石头的机器学习之路（ID：redstonewill）**"]],[0,19],[18127,18127]]]]]]}