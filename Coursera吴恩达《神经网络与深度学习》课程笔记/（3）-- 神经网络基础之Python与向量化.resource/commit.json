{"compress":true,"commitItems":[["5e937341-1b06-4d1a-8a7d-db4744bd7865",1525490282153,"",[[1525490252530,["gao@gggg",[[1,0,"> 我的CSDN博客地址：[红色石头的专栏](https://link.zhihu.com/?target=http%3A//blog.csdn.net/red_stone1)\n> 我的知乎主页：[红色石头](https://www.zhihu.com/people/red_stone_wl)\n> 我的微博：[RedstoneWill的微博](https://link.zhihu.com/?target=https%3A//weibo.com/6479023696/profile%3Ftopnav%3D1%26wvr%3D6%26is_all%3D1)\n> 我的GitHub：[RedstoneWill的GitHub](https://link.zhihu.com/?target=https%3A//github.com/RedstoneWill)\n> 我的微信公众号：红色石头的机器学习之路（ID：redstonewill）\n> 欢迎大家关注我！共同学习，共同进步！\n\n上节课我们主要介绍了逻辑回归，以输出概率的形式来处理二分类问题。我们介绍了逻辑回归的Cost function表达式，并使用梯度下降算法来计算最小化Cost function时对应的参数w和b。通过计算图的方式来讲述了神经网络的正向传播和反向传播两个过程。本节课我们将来探讨Python和向量化的相关知识。\n\n## **1\\. Vectorization**\n\n深度学习算法中，数据量很大，在程序中应该尽量减少使用loop循环语句，而可以使用向量运算来提高程序运行速度。\n\n向量化（Vectorization）就是利用矩阵运算的思想，大大提高运算速度。例如下面所示在Python中使用向量化要比使用循环计算速度快得多。\n\n```python3\nimport numpy as np\nimport time\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\n\ntic = time.time()\nc = np.dot(a,b)\ntoc = time.time()\n\nprint(c)\nprint(\"Vectorized version:\" + str(1000*(toc-tic)) + \"ms\")\n\nc = 0\ntic = time.time()\nfor i in range(1000000):\n\tc += a[i]*b[i]\ntoc = time.time()\n\nprint(c)\nprint(\"for loop:\" + str(1000*(toc-tic)) + \"ms\")\n\n```\n\n输出结果类似于：\n\n```text\n250286.989866\nVectorized version:1.5027523040771484ms\n250286.989866\nFor loop:474.29513931274414ms\n\n```\n\n从程序运行结果上来看，该例子使用for循环运行时间是使用向量运算运行时间的约300倍。因此，深度学习算法中，使用向量化矩阵运算的效率要高得多。\n\n为了加快深度学习神经网络运算速度，可以使用比CPU运算能力更强大的GPU。事实上，GPU和CPU都有并行指令（parallelization instructions），称为Single Instruction Multiple Data（SIMD）。SIMD是单指令多数据流，能够复制多个操作数，并把它们打包在大型寄存器的一组指令集。SIMD能够大大提高程序运行速度，例如python的numpy库中的内建函数（built-in function）就是使用了SIMD指令。相比而言，GPU的SIMD要比CPU更强大一些。\n\n## **2\\. More Vectorization Examples**\n\n上一部分我们讲了应该尽量避免使用for循环而使用向量化矩阵运算。在python的numpy库中，我们通常使用np.dot()函数来进行矩阵运算。\n\n我们将向量化的思想使用在逻辑回归算法上，尽可能减少for循环，而只使用矩阵运算。值得注意的是，算法最顶层的迭代训练的for循环是不能替换的。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。\n\n## **3\\. Vectorizing Logistic Regression**\n\n在《神经网络与深度学习》课程笔记（2）中我们介绍过，整个训练样本构成的输入矩阵X的维度是（ ![n_x ](https://www.zhihu.com/equation?tex=n_x+) ，m），权重矩阵w的维度是（ ![n_x](https://www.zhihu.com/equation?tex=n_x) ，1），b是一个常数值，而整个训练样本构成的输出矩阵Y的维度为（1，m）。利用向量化的思想，所有m个样本的线性输出Z可以用矩阵表示：\n\n![Z=w^TX+b](https://www.zhihu.com/equation?tex=Z%3Dw%5ETX%2Bb)\n\n在python的numpy库中可以表示为：\n\n```python3\nZ = np.dot(w.T,X) + b\nA = sigmoid(Z)\n\n```\n\n其中，w.T表示w的转置。\n\n这样，我们就能够使用向量化矩阵运算代替for循环，对所有m个样本同时运算，大大提高了运算速度。\n\n## **4\\. Vectorizing Logistic Regression’s Gradient Output**\n\n再来看逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有m个样本，dZ的维度是（1，m），可表示为：\n\n![dZ=A-Y](https://www.zhihu.com/equation?tex=dZ%3DA-Y)\n\ndb可表示为：\n\n![db=\\frac1m \\sum_{i=1}^mdz^{(i)}](https://www.zhihu.com/equation?tex=db%3D%5Cfrac1m+%5Csum_%7Bi%3D1%7D%5Emdz%5E%7B%28i%29%7D)\n\n对应的程序为：\n\n```text\ndb = 1/m*np.sum(dZ)\n\n```\n\ndw可表示为：\n\n![dw=\\frac1m X\\cdot dZ^T](https://www.zhihu.com/equation?tex=dw%3D%5Cfrac1m+X%5Ccdot+dZ%5ET)\n\n对应的程序为：\n\n```python3\ndw = 1/m*np.dot(X,dZ.T)\n\n```\n\n这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：\n\n```text\nZ = np.dot(w.T,X) + b\nA = sigmoid(Z)\ndZ = A-Y\ndw = 1/m*np.dot(X,dZ.T)\ndb = 1/m*np.sum(dZ)\n\nw = w - alpha*dw\nb = b - alpha*db\n\n```\n\n其中，alpha是学习因子，决定w和b的更新速度。上述代码只是对单次训练更新而言的，外层还需要一个for循环，表示迭代次数。\n\n## **5\\. Broadcasting in Python**\n\n下面介绍使用python的另一种技巧：广播（Broadcasting）。python中的广播机制可由下面四条表示：\n\n*   **让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐**\n*   **输出数组的shape是输入数组shape的各个轴上的最大值**\n*   **如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错**\n*   **当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值**\n\n简而言之，就是python中可以对不同维度的矩阵进行四则混合运算，但至少保证有一个维度是相同的。下面给出几个广播的例子，具体细节可参阅python的相关手册，这里就不赘述了。\n\n![](https://pic2.zhimg.com/80/v2-3f445018e57eb0885a14607d079bb0c9_hd.jpg)\n\n值得一提的是，在python程序中为了保证矩阵运算正确，可以使用reshape()函数来对矩阵设定所需的维度。这是一个很好且有用的习惯。\n\n## **6\\. A note on python/numpy vectors**\n\n接下来我们将总结一些python的小技巧，避免不必要的code bug。\n\npython中，如果我们用下列语句来定义一个向量：\n\n```text\na = np.random.randn(5)\n\n```\n\n这条语句生成的a的维度是（5，）。它既不是行向量也不是列向量，我们把a叫做rank 1 array。这种定义会带来一些问题。例如我们对a进行转置，还是会得到a本身。所以，如果我们要定义（5，1）的列向量或者（1，5）的行向量，最好使用下来标准语句，避免使用rank 1 array。\n\n```text\na = np.random.randn(5,1)\nb = np.random.randn(1,5)\n\n```\n\n除此之外，我们还可以使用assert语句对向量或数组的维度进行判断，例如：\n\n```text\nassert(a.shape == (5,1))\n\n```\n\nassert会对内嵌语句进行判断，即判断a的维度是不是（5，1）的。如果不是，则程序在此处停止。使用assert语句也是一种很好的习惯，能够帮助我们及时检查、发现语句是否正确。\n\n另外，还可以使用reshape函数对数组设定所需的维度：\n\n```text\na.reshape((5,1))\n\n```\n\n## **7\\. Quick tour of Jupyter/iPython Notebooks**\n\nJupyter notebook（又称IPython notebook）是一个交互式的笔记本，支持运行超过40种编程语言。本课程所有的编程练习题都将在Jupyter notebook上进行，使用的语言是python。\n\n关于Jupyter notebook的简介和使用方法可以看我的另外两篇博客：\n\n[Jupyter notebook入门教程（上）](https://link.zhihu.com/?target=http%3A//blog.csdn.net/red_stone1/article/details/72858962)\n\n[Jupyter notebook入门教程（下）](https://link.zhihu.com/?target=http%3A//blog.csdn.net/red_stone1/article/details/72863749)\n\n## **8\\. Explanation of logistic regression cost function(optional)**\n\n在上一节课的笔记中，我们介绍过逻辑回归的Cost function。接下来我们将简要解释这个Cost function是怎么来的。\n\n首先，预测输出 ![\\hat y](https://www.zhihu.com/equation?tex=%5Chat+y) 的表达式可以写成：\n\n![hat y=\\sigma(w^Tx+b)](https://www.zhihu.com/equation?tex=hat+y%3D%5Csigma%28w%5ETx%2Bb%29)\n\n其中， ![\\sigma(z)=\\frac{1}{1+exp(-z)}](https://www.zhihu.com/equation?tex=%5Csigma%28z%29%3D%5Cfrac%7B1%7D%7B1%2Bexp%28-z%29%7D) 。 ![\\hat y](https://www.zhihu.com/equation?tex=%5Chat+y) 可以看成是预测输出为正类（+1）的概率：\n\n![\\hat y=P(y=1|x)](https://www.zhihu.com/equation?tex=%5Chat+y%3DP%28y%3D1%7Cx%29)\n\n那么，当y=1时：\n\n![p(y|x)=\\hat y](https://www.zhihu.com/equation?tex=p%28y%7Cx%29%3D%5Chat+y)\n\n当y=0时：\n\n![p(y|x)=1-\\hat y](https://www.zhihu.com/equation?tex=p%28y%7Cx%29%3D1-%5Chat+y)\n\n我们把上面两个式子整合到一个式子中，得到：\n\n![P(y|x)=\\hat y^y(1-\\hat y)^{(1-y)}](https://www.zhihu.com/equation?tex=P%28y%7Cx%29%3D%5Chat+y%5Ey%281-%5Chat+y%29%5E%7B%281-y%29%7D)\n\n由于log函数的单调性，可以对上式P(y|x)进行log处理：\n\n![log\\ P(y|x)=log\\ \\hat y^y(1-\\hat y)^{(1-y)}=y\\ log\\ \\hat y+(1-y)log(1-\\hat y)](https://www.zhihu.com/equation?tex=log%5C+P%28y%7Cx%29%3Dlog%5C+%5Chat+y%5Ey%281-%5Chat+y%29%5E%7B%281-y%29%7D%3Dy%5C+log%5C+%5Chat+y%2B%281-y%29log%281-%5Chat+y%29)\n\n我们希望上述概率P(y|x)越大越好，对上式加上负号，则转化成了单个样本的Loss function，越小越好，也就得到了我们之前介绍的逻辑回归的Loss function形式。\n\n![L=-(y\\ log\\ \\hat y+(1-y)log(1-\\hat y))](https://www.zhihu.com/equation?tex=L%3D-%28y%5C+log%5C+%5Chat+y%2B%281-y%29log%281-%5Chat+y%29%29)\n\n如果对于所有m个训练样本，假设样本之间是独立同分布的（iid），我们希望总的概率越大越好：\n\n![max\\ \\prod_{i=1}^m\\ P(y^{(i)}|x^{(i)})](https://www.zhihu.com/equation?tex=max%5C+%5Cprod_%7Bi%3D1%7D%5Em%5C+P%28y%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%29)\n\n同样引入log函数，加上负号，将上式转化为Cost function：\n\n![J(w,b)=-\\frac1m\\sum_{i=1}^mL(\\hat y^{(i)},y^{(i)})=- \\frac1m\\sum_{i=1}^my^{(i)}\\ log\\ \\hat y^{(i)}+(1-y^{(i)})log(1-\\hat y^{(i)})](https://www.zhihu.com/equation?tex=J%28w%2Cb%29%3D-%5Cfrac1m%5Csum_%7Bi%3D1%7D%5EmL%28%5Chat+y%5E%7B%28i%29%7D%2Cy%5E%7B%28i%29%7D%29%3D-+%5Cfrac1m%5Csum_%7Bi%3D1%7D%5Emy%5E%7B%28i%29%7D%5C+log%5C+%5Chat+y%5E%7B%28i%29%7D%2B%281-y%5E%7B%28i%29%7D%29log%281-%5Chat+y%5E%7B%28i%29%7D%29)\n\n上式中， ![\\frac1m](https://www.zhihu.com/equation?tex=%5Cfrac1m) 表示对所有m个样本的Cost function求平均，是缩放因子。\n\n## **9\\. Summary**\n\n本节课我们主要介绍了神经网络基础——python和向量化。在深度学习程序中，使用向量化和矩阵运算的方法能够大大提高运行速度，节省时间。以逻辑回归为例，我们将其算法流程包括梯度下降转换为向量化的形式。同时，我们也介绍了python的相关编程方法和技巧。\n\n**_我的CSDN博客地址：_**\n\n[Coursera吴恩达《神经网络与深度学习》课程笔记（3）-- 神经网络基础之Python与向量化](https://link.zhihu.com/?target=http%3A//blog.csdn.net/red_stone1/article/details/77929889)\n\n**更多AI资源请关注公众号：红色石头的机器学习之路（ID：redstonewill）**"]],[0,0],[7130,7130]]]]]]}