{"compress":true,"commitItems":[["260d4b2d-aa47-4228-b6f3-3d74ad5ee55e",1525490784180,"",[[1525490768223,["gao@gggg",[[1,0,"（2）-- 优化算法\n===\n\n\n"]],[0,0],[17,17]]],[1525490782340,["gao@gggg",[[1,0,"> 我的CSDN博客地址：[红色石头的专栏](https://link.zhihu.com/?target=http%3A//blog.csdn.net/red_stone1)\n> 我的知乎主页：[红色石头](https://www.zhihu.com/people/red_stone_wl)\n> 我的微博：[RedstoneWill的微博](https://link.zhihu.com/?target=https%3A//weibo.com/6479023696/profile%3Ftopnav%3D1%26wvr%3D6%26is_all%3D1)\n> 我的GitHub：[RedstoneWill的GitHub](https://link.zhihu.com/?target=https%3A//github.com/RedstoneWill)\n> 我的微信公众号：红色石头的机器学习之路"],[1,1,"ID：redstonewill）\n> 欢迎大家关注我！共同学习，共同进步！\n\n上节课我们主要介绍了如何建立一个实用的深度学习神经网络。包括Train/Dev/Test sets的比例选择，Bias和Variance的概念和区别：Bias对应欠拟合，Variance对应过拟合。接着，我们介绍了防止过拟合的两种方法：L"],[1,2," regularization和Dropout。然后，介绍了如何进行规范化输入，以加快梯度下降速度和精度。然后，我们介绍了梯度消失和梯度爆炸的概念和危害，并提出了如何使用梯度初始化来降低这种风险。最后，我们介绍了梯度检查，来验证梯度下降算法是否正确。本节课，我们将继续讨论深度神经网络中的一些优化算法，通过使用这些技巧和方法来提高神经网络的训练速度和精度。\n\n## **1\\. Mini-batch gradient descent**\n\n之前我们介绍的神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式，同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为Batch Gradient Descent。\n\n为了解决这一问题，我们可以把m个训练样本分成若干个子集，称为mini-batches，这样每个子集包含的数据量就小了，例如只有1000，然后每次在单一子集上进行神经网络训练，速度就会大大提高。这种梯度下降算法叫做Mini-batch Gradient Descent。\n\n假设总的训练样本个数m=5000000，其维度为 ![(n_x,m)](https://www.zhihu.com/equation?tex=%28n_x%2Cm%29) 。将其分成5000个子集，每个mini-batch含有1000个样本。我们将每个mini-batch记为 ![X^{\\{t\\}}](https://www.zhihu.com/equation?tex=X%5E%7B%5C%7Bt%5C%7D%7D) ，其维度为 ![(n_x,1000)](https://www.zhihu.com/equation?tex=%28n_x%2C1000%29) 。相应的每个mini-batch的输出记为 ![Y^{\\{t\\}}](https://www.zhihu.com/equation?tex=Y%5E%7B%5C%7Bt%5C%7D%7D) ，其维度为 ![(1,1000)](https://www.zhihu.com/equation?tex=%281%2C1000%29) ，且 ![t=1,2,\\cdots,5000](https://www.zhihu.com/equation?tex=t%3D1%2C2%2C%5Ccdots%2C5000) 。\n\n这里顺便总结一下我们遇到的神经网络中几类字母的上标含义：\n\n*   ![X^{(i)}](https://www.zhihu.com/equation?tex=X%5E%7B%28i%29%7D) **：第i个样本**\n*   ![Z^{[l]}](https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D) **：神经网络第l层网络的线性输出**\n*   ![X^{\\{t\\}},Y^{\\{t\\}}](https://www.zhihu.com/equation?tex=X%5E%7B%5C%7Bt%5C%7D%7D%2CY%5E%7B%5C%7Bt%5C%7D%7D) **：第t组mini-batch**\n\nMini-batches Gradient Descent的实现过程是先将总的训练样本分成T个子集（mini-batches"],[1,3,"，然后对每个mini"],[1,4,"batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini"],[1,5,"batch都训练完毕。\n\n![for\\"],[1,6,"\\ t=1,\\cdots,T\\ \\ \\{](https://www.zhihu.com/equation?tex=for%5C+%5C+t%3D1%2C%5Ccdots%2CT%5C+%5C+%5C%7B)\n\n![\\ \\ \\ \\ Forward\\ Propagation](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+Forward%5C+Propagation)\n\n![\\ \\ \\ \\ Compute Cost Function](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+Compute+Cost+Function)\n\n![\\ \\ \\ \\ Backward Propagation](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+Backward+Propagation)\n\n![\\ \\ \\ \\ W:=W-\\alpha\\cdot dW](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+W%3A%3DW-%5Calpha%5Ccdot+dW)\n\n![\\ \\ \\ \\ b:=b-\\alpha\\cdot db](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+b%3A%3Db-%5Calpha%5Ccdot+db)\n\n![\\}](https://www.zhihu.com/equation?tex=%5C%7D)\n\n经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个epoch。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法。\n\n值得一提的是，对于Mini-Batches Gradient Descent，可以进行多次epoch训练。而且，每次epoch，最好是将总体训练数据重新打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型。\n\n## **2\\. Understanding mini-batch gradient descent**\n\nBatch gradient descent和Mini-batch gradient descent的cost曲线如下图所示：\n\n![](https://pic3.zhimg.com/80/v2-4cd211474fdc2b1c5643b80de3144bb3_hd.jpg)\n\n对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。然而，使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值。\n\n之所以出现细微振荡的原因是不同的mini-batch之间是有差异的。例如可能第一个子集 ![(X^{\\{1\\}},Y^{\\{1\\}})](https://www.zhihu.com/equation?tex=%28X%5E%7B%5C%7B1%5C%7D%7D%2CY%5E%7B%5C%7B1%5C%7D%7D%29) 是好的子集，而第二个子集 ![(X^{\\{2\\}},Y^{\\{2\\}})](https://www.zhihu.com/equation?tex=%28X%5E%7B%5C%7B2%5C%7D%7D%2CY%5E%7B%5C%7B2%5C%7D%7D%29) 包含了一些噪声noise。出现细微振荡是正常的。\n\n如何选择每个mini-batch的大小，即包含的样本个数呢？有两个极端：如果mini-batch size=m，即为Batch gradient descent，只包含一个子集为 ![(X^{\\{1\\}},Y^{\\{1\\}})=(X,Y)](https://www.zhihu.com/equation?tex=%28X%5E%7B%5C%7B1%5C%7D%7D%2CY%5E%7B%5C%7B1%5C%7D%7D%29%3D%28X%2CY%29) ；如果mini-batch size=1，即为Stachastic gradient descent，每个样本就是一个子集 ![(X^{\\{1\\}},Y^{\\{1\\}})=(x^{(i)},y^{(i)})](https://www.zhihu.com/equation?tex=%28X%5E%7B%5C%7B1%5C%7D%7D%2CY%5E%7B%5C%7B1%5C%7D%7D%29%3D%28x%5E%7B%28i%29%7D%2Cy%5E%7B%28i%29%7D%29) ，共有m个子集。\n\n我们来比较一下Batch gradient descent和Stachastic gradient descent的梯度下降曲线。如下图所示，蓝色的线代表Batch gradient descent，紫色的线代表Stachastic gradient descent。Batch gradient descent会比较平稳地接近全局最小值，但是因为使用了所有m个样本，每次前进的速度有些慢。Stachastic gradient descent每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。\n\n![](https://pic4.zhimg.com/80/v2-acb9dce0b8987417dd898c7b9922c17c_hd.jpg)\n\n实际使用中，mini-batch size不能设置得太大（Batch gradient descent），也不能设置得太小（Stachastic gradient descent）。这样，相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，既能使用向量化"],[1,10,"，又能叫快速地找到最小值。mini-batch gradient descent的梯度下降曲线如下图绿色所示，每次前进速度较快，且振荡较小，基本能接近全局最小值。\n"],[1,11,"![](https://pic3.zhimg.com/80/v2-3d2a8a956bce93e9267b63323b1c48ca_hd.jpg)\n\n一般来说，如果总体样本数量m不太大时，例如 ![m\\leq2000](https://www.zhihu.com/equation?tex"],[1,12,"m%5Cleq2000) ，建议直接使用Batch gradient descent。如果总体样本数量m很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为64,128,256,512。这些都是2的幂。之所以这样设置的原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度。\n\n## **3\\. Exponentially weighted averages**\n\n该部分我们将介绍指数加权平均（Exponentially weighted averages）的概念。\n\n举个例子，记录半年内伦敦市的气温变化，并在二维平面上绘制出来，如下图所示：\n\n![](https://pic2.zhimg.com/80/v2-0ae2f94a905e3a0ef04cf7c0fdb73238_hd.jpg)\n\n看上去，温度数据似乎有noise，而且抖动较大。如果我们希望看到半年内气温的整体变化趋势，可以通过移动平均（moving average）的方法来对每天气温进行平滑处理。\n\n例如我们可以设 ![V_0"],[1,13,"0](https://www.zhihu.com/equation?tex"],[1,14,"V_0%3D0) ，当成第0天的气温值。"],[1,16,"第一天的气温与第0天的气温有关："],[1,17,"\n![V_1=0.9V_0+0.1\\theta_1](https://www.zhihu.com/equation?tex=V_1%3D0.9V_0%2B0.1%5Ctheta_1)\n\n第二天的气温与第一天的气温有关：\n\n![\\begin{eqnarray}V_2 &=&0.9V_1+0.1\\theta_2\\\\ &=&0.9(0.9V_0+0.1\\theta_1)+0.1\\theta_2\\\\ &=&0.9^2V_0+0.9\\cdot0.1\\theta_1+0.1\\theta_2 \\end{eqnarray}](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7DV_2+%26%3D%260.9V_1%2B0.1%5Ctheta_2%5C%5C+%26%3D%260.9%280.9V_0%2B0.1%5Ctheta_1%29%2B0.1%5Ctheta_2%5C%5C+%26%3D%260.9%5E2V_0%2B0.9%5Ccdot0.1%5Ctheta_1%2B0.1%5Ctheta_2+%5Cend%7Beqnarray%7D)\n\n第三天的气温与第二天的气温有关：\n\n![\\begin{eqnarray}V_3 &=&0.9V_2+0.1\\theta_3\\\\ &=&0.9(0.9^2V_0+0.9\\cdot0.1\\theta_1+0.1\\theta_2)+0.1\\theta_3\\\\ &=&0.9^3V_0+0.9^2\\cdot 0.1\\theta_1+0.9\\cdot 0.1\\theta_2+0.1\\theta_3 \\end{eqnarray}](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7DV_3+%26%3D%260.9V_2%2B0.1%5Ctheta_3%5C%5C+%26%3D%260.9%280.9%5E2V_0%2B0.9%5Ccdot0.1%5Ctheta_1%2B0.1%5Ctheta_2%29%2B0.1%5Ctheta_3%5C%5C+%26%3D%260.9%5E3V_0%2B0.9%5E2%5Ccdot+0.1%5Ctheta_1%2B0.9%5Ccdot+0.1%5Ctheta_2%2B0.1%5Ctheta_3+%5Cend%7Beqnarray%7D)\n\n即第t天与第t-1天的气温迭代关系为：\n\n![\\begin{eqnarray}V_t &=&0.9V_{t-1}+0.1\\theta_t\\\\ &=&0.9^tV_0+0.9^{t-1}\\cdot0.1\\theta_1+0.9^{t-2}\\cdot 0.1\\theta_2+\\cdots+0.9\\cdot0.1\\theta_{t-1}+0.1\\theta_t \\end{eqnarray}](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7DV_t+%26%3D%260.9V_%7Bt-1%7D%2B0.1%5Ctheta_t%5C%5C+%26%3D%260.9%5EtV_0%2B0.9%5E%7Bt-1%7D%5Ccdot0.1%5Ctheta_1%2B0.9%5E%7Bt-2%7D%5Ccdot+0.1%5Ctheta_2%2B%5Ccdots%2B0.9%5Ccdot0.1%5Ctheta_%7Bt-1%7D%2B0.1%5Ctheta_t+%5Cend%7Beqnarray%7D)\n\n经过移动平均处理得到的气温如下图红色曲线所示：\n\n![](https://pic1.zhimg.com/80/v2-f7d3e7925fce0db64e4ef6f58b9c65b2_hd.jpg)\n\n这种滑动平均算法称为指数加权平均（exponentially weighted average）。根据之前的推导公式，其一般形式为：\n\n![V_t=\\beta V_{t-1}+(1-\\beta)\\theta_t](https://www.zhihu.com/equation?tex=V_t%3D%5Cbeta+V_%7Bt-1%7D%2B%281-%5Cbeta%29%5Ctheta_t)\n\n上面的例子中， ![\\beta=0.9](https://www.zhihu.com/equation?tex=%5Cbeta%3D0.9) 。 ![\\beta](https://www.zhihu.com/equation?tex=%5Cbeta) 值决定了指数加权平均的天数，近似表示为：\n\n![\\frac{1}{1-\\beta}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D)\n\n例如，当 ![\\beta=0.9](https://www.zhihu.com/equation?tex=%5Cbeta%3D0.9) ，则 ![\\frac{1}{1-\\beta}=10](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D%3D10) ，表示将前10天进行指数加权平均。当 ![\\beta=0.98](https://www.zhihu.com/equation?tex=%5Cbeta%3D0.98) ，则 ![\\frac{1}{1-\\beta}=50](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D%3D50) ，表示将前50天进行指数加权平均。 ![\\beta](https://www.zhihu.com/equation?tex=%5Cbeta) 值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，但是同时也会向右平移。下图绿色曲线和黄色曲线分别表示了 ![\\beta=0.98](https://www.zhihu.com/equation?tex=%5Cbeta%3D0.98) 和 ![\\beta=0.5](https://www.zhihu.com/equation?tex=%5Cbeta%3D0.5) 时，指数加权平均的结果。\n\n![](https://pic4.zhimg.com/80/v2-ed09d0dbdf3329254cf53657dc6b914a_hd.jpg)\n\n这里简单解释一下公式 ![\\frac{1}{1-\\beta}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D) 是怎么来的。准确来说，指数加权平均算法跟之前所有天的数值都有关系，根据之前的推导公式就能看出。但是指数是衰减的，一般认为衰减到 ![\\frac1e](https://www.zhihu.com/equation?tex=%5Cfrac1e) 就可以忽略不计了。因此，根据之前的推导公式，我们只要证明\n\n![\\beta^{\\frac{1}{1-\\beta}}=\\frac1e](https://www.zhihu.com/equation?tex=%5Cbeta%5E%7B%5Cfrac%7B1%7D%7B1-%5Cbeta%7D%7D%3D%5Cfrac1e)\n\n就好了。\n\n令 ![\\frac{1}{1-\\beta}=N](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D%3DN) ， ![N>0](https://www.zhihu.com/equation?tex=N%3E0) ，则 ![\\beta=1-\\frac{1}{N}](https://www.zhihu.com/equation?tex=%5Cbeta%3D1-%5Cfrac%7B1%7D%7BN%7D) ， ![\\frac1N<1](https://www.zhihu.com/equation?tex=%5Cfrac1N%3C1) 。即证明转化为：\n\n![(1-\\frac1N)^N=\\frac1e](https://www.zhihu.com/equation?tex=%281-%5Cfrac1N%29%5EN%3D%5Cfrac1e)\n\n显然，当 ![N>>0](https://www.zhihu.com/equation?tex=N%3E%3E0) 时，上述等式是近似成立的。\n\n至此，简单解释了为什么指数加权平均的天数的计算公式为 ![\\frac{1}{1-\\beta}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D) 。\n\n## **4\\. Understanding exponetially weighted averages**\n\n我们将指数加权平均公式的一般形式写下来：\n\n![\\begin{eqnarray}V_t &=&\\beta V_{t-1}+(1-\\beta)\\theta_t\\\\ &=&(1-\\beta)\\theta_t+(1-\\beta)\\cdot\\beta\\cdot\\theta_{t-1}+(1-\\beta)\\cdot \\beta^2\\cdot\\theta_{t-2}+\\cdots\\\\ &&+(1-\\beta)\\cdot \\beta^{t-1}\\cdot \\theta_1+\\beta^t\\cdot V_0 \\end{eqnarray}](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7DV_t+%26%3D%26%5Cbeta+V_%7Bt-1%7D%2B%281-%5Cbeta%29%5Ctheta_t%5C%5C+%26%3D%26%281-%5Cbeta%29%5Ctheta_t%2B%281-%5Cbeta%29%5Ccdot%5Cbeta%5Ccdot%5Ctheta_%7Bt-1%7D%2B%281-%5Cbeta%29%5Ccdot+%5Cbeta%5E2%5Ccdot%5Ctheta_%7Bt-2%7D%2B%5Ccdots%5C%5C+%26%26%2B%281-%5Cbeta%29%5Ccdot+%5Cbeta%5E%7Bt-1%7D%5Ccdot+%5Ctheta_1%2B%5Cbeta%5Et%5Ccdot+V_0+%5Cend%7Beqnarray%7D)\n\n观察上面这个式子， ![\\theta_t,\\theta_{t-1},\\theta_{t-2},\\cdots,\\theta_1](https://www.zhihu.com/equation?tex=%5Ctheta_t%2C%5Ctheta_%7Bt-1%7D%2C%5Ctheta_%7Bt-2%7D%2C%5Ccdots%2C%5Ctheta_1) 原始数据值， ![(1-\\beta),(1-\\beta)\\beta,(1-\\beta)\\beta^2,\\cdots,(1-\\beta)\\beta^{t-1}](https://www.zhihu.com/equation?tex=%281-%5Cbeta%29%2C%281-%5Cbeta%29%5Cbeta%2C%281-%5Cbeta%29%5Cbeta%5E2%2C%5Ccdots%2C%281-%5Cbeta%29%5Cbeta%5E%7Bt-1%7D) 是类似指数曲线，从右向左，呈指数下降的。 ![V_t](https://www.zhihu.com/equation?tex=V_t) 的值就是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害。\n\n![](https://pic4.zhimg.com/80/v2-6d1f3025d3b701bdecc0b49df560f0eb_hd.jpg)\n\n我们已经知道了指数加权平均的递推公式。实际应用中，为了减少内存的使用，我们可以使用这样的语句来实现指数加权平均算法：\n\n![V_{\\theta}=0](https://www.zhihu.com/equation?tex=V_%7B%5Ctheta%7D%3D0)\n\n![Repeat\\ \\{](https://www.zhihu.com/equation?tex=Repeat%5C+%5C%7B)\n\n![\\ \\ \\ \\ Get\\ next\\ \\theta_t](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+Get%5C+next%5C+%5Ctheta_t)\n\n![\\ \\ \\ \\ V_{\\theta}:=\\beta V_{\\theta}+(1-\\beta)\\theta_t](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+V_%7B%5Ctheta%7D%3A%3D%5Cbeta+V_%7B%5Ctheta%7D%2B%281-%5Cbeta%29%5Ctheta_t)\n\n![\\}](https://www.zhihu.com/equation?tex=%5C%7D)\n\n## **5\\. Bias correction in exponentially weighted average**\n\n上文中提到当 ![\\beta=0.98](https://www.zhihu.com/equation?tex=%5Cbeta%3D0.98) 时，指数加权平均结果如下图绿色曲线所示。但是实际上，真实曲线如紫色曲线所示。\n\n![](https://pic3.zhimg.com/80/v2-308ae272b30db39820af2065934b3346_hd.jpg)\n\n我们注意到，紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。这是因为开始时我们设置 ![V_0=0](https://www.zhihu.com/equation?tex=V_0%3D0) ，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常。\n\n修正这种问题的方法是进行偏移校正（bias correction），即在每次计算完 ![V_t](https://www.zhihu.com/equation?tex=V_t) 后，对 ![V_t](https://www.zhihu.com/equation?tex=V_t) 进行下式处理：\n\n![\\frac{V_t}{1-\\beta^t}](https://www.zhihu.com/equation?tex=%5Cfrac%7BV_t%7D%7B1-%5Cbeta%5Et%7D)\n\n在刚开始的时候，t比较小， ![(1-\\beta^t)<1](https://www.zhihu.com/equation?tex=%281-%5Cbeta%5Et%29%3C1) ，这样就将 ![V_t](https://www.zhihu.com/equation?tex=V_t) 修正得更大一些，效果是把紫色曲线开始部分向上提升一些，与绿色曲线接近重合。随着t增大， ![(1-\\beta^t)\\approx1](https://www.zhihu.com/equation?tex=%281-%5Cbeta%5Et%29%5Capprox1) ， ![V_t](https://www.zhihu.com/equation?tex=V_t) 基本不变，紫色曲线与绿色曲线依然重合。这样就实现了简单的偏移校正，得到我们希望的绿色曲线。\n\n值得一提的是，机器学习中，偏移校正并不是必须的。因为，在迭代一次次数后（t较大）， ![V_t](https://www.zhihu.com/equation?tex=V_t) 受初始值影响微乎其微，紫色曲线与绿色曲线基本重合。所以，一般可以忽略初始迭代过程，等到一定迭代之后再取值，这样就不需要进行偏移校正了。\n\n## **6\\. Gradient descent with momentum**\n\n该部分将介绍动量梯度下降算法，其速度要比传统的梯度下降算法快很多。做法是在每次训练时，对梯度进行指数加权平均处理，然后用得到的梯度值更新权重W和常数项b。下面介绍具体的实现过程。\n\n![](https://pic3.zhimg.com/80/v2-f0a77a3a985b7d0ee1ebe79c125ff4d3_hd.jpg)\n\n原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，梯度下降的振荡较大，尤其对于W、b之间数值范围差别较大的情况。此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。而如果对梯度进行指数加权平均，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处。\n\n权重W和常数项b的指数加权平均表达式如下：\n\n![V_{dW}=\\beta\\cdot V_{dW}+(1-\\beta)\\cdot dW](https://www.zhihu.com/equation?tex=V_%7BdW%7D%3D%5Cbeta%5Ccdot+V_%7BdW%7D%2B%281-%5Cbeta%29%5Ccdot+dW)\n\n![V_{db}=\\beta\\cdot V_{db}+(1-\\beta)\\cdot db](https://www.zhihu.com/equation?tex=V_%7Bdb%7D%3D%5Cbeta%5Ccdot+V_%7Bdb%7D%2B%281-%5Cbeta%29%5Ccdot+db)\n\n从动量的角度来看，以权重W为例， ![V_{dW}](https://www.zhihu.com/equation?tex=V_%7BdW%7D) 可以成速度V， ![dW](https://www.zhihu.com/equation?tex=dW) 可以看成是加速度a。指数加权平均实际上是计算当前的速度，当前速度由之前的速度和现在的加速度共同影响。而 ![\\beta<1](https://www.zhihu.com/equation?tex=%5Cbeta%3C1) ，又能限制速度 ![V_{dW}](https://www.zhihu.com/equation?tex=V_%7BdW%7D) 过大。也就是说，当前的速度是渐变的，而不是瞬变的，是动量的过程。这保证了梯度下降的平稳性和准确性，减少振荡，较快地达到最小值处。\n\n动量梯度下降算法的过程如下：\n\n![On\\ iteration\\ t:](https://www.zhihu.com/equation?tex=On%5C+iteration%5C+t%3A)\n\n![\\ \\ \\ \\ Compute\\ dW,\\ db\\ on\\ the\\ current\\ mini-batch](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+Compute%5C+dW%2C%5C+db%5C+on%5C+the%5C+current%5C+mini-batch)\n\n![\\ \\ \\ \\ V_{dW}=\\beta V_{dW}+(1-\\beta)dW](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+V_%7BdW%7D%3D%5Cbeta+V_%7BdW%7D%2B%281-%5Cbeta%29dW)\n\n![\\ \\ \\ \\ V_{db}=\\beta V_{db}+(1-\\beta)db](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+V_%7Bdb%7D%3D%5Cbeta+V_%7Bdb%7D%2B%281-%5Cbeta%29db)\n\n![\\ \\ \\ \\ W=W-\\alpha V_{dW},\\ b=b-\\alpha V_{db}](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+W%3DW-%5Calpha+V_%7BdW%7D%2C%5C+b%3Db-%5Calpha+V_%7Bdb%7D)\n\n初始时，令 ![V_{dW}=0,V_{db}=0](https://www.zhihu.com/equation?tex=V_%7BdW%7D%3D0%2CV_%7Bdb%7D%3D0) 。一般设置 ![\\beta=0.9](https://www.zhihu.com/equation?tex=%5Cbeta%3D0.9) ，即指数加权平均前10天的数据，实际应用效果较好。\n\n另外，关于偏移校正，可以不使用。因为经过10次迭代后，随着滑动平均的过程，偏移情况会逐渐消失。\n\n补充一下，在其它文献资料中，动量梯度下降还有另外一种写法：\n\n![V_{dW}=\\beta V_{dW}+dW](https://www.zhihu.com/equation?tex=V_%7BdW%7D%3D%5Cbeta+V_%7BdW%7D%2BdW)\n\n![V_{db}=\\beta V_{db}+db](https://www.zhihu.com/equation?tex=V_%7Bdb%7D%3D%5Cbeta+V_%7Bdb%7D%2Bdb)\n\n即消去了 ![dW](https://www.zhihu.com/equation?tex=dW) 和 ![db](https://www.zhihu.com/equation?tex=db) 前的系数 ![(1-\\beta)](https://www.zhihu.com/equation?tex=%281-%5Cbeta%29) 。这样简化了表达式，但是学习因子 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 相当于变成了 ![\\frac{\\alpha}{1-\\beta}](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Calpha%7D%7B1-%5Cbeta%7D) ，表示 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 也受 ![\\beta](https://www.zhihu.com/equation?tex=%5Cbeta) 的影响。从效果上来说，这种写法也是可以的，但是不够直观，且调参涉及到 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) ，不够方便。所以，实际应用中，推荐第一种动量梯度下降的表达式。\n\n## **7\\. RMSprop**\n\nRMSprop是另外一种优化梯度下降速度的算法。每次迭代训练过程中，其权重W和常数项b的更新表达式为：\n\n![S_W=\\beta S_{dW}+(1-\\beta)dW^2](https://www.zhihu.com/equation?tex=S_W%3D%5Cbeta+S_%7BdW%7D%2B%281-%5Cbeta%29dW%5E2)\n\n![S_b=\\beta S_{db}+(1-\\beta)db^2](https://www.zhihu.com/equation?tex=S_b%3D%5Cbeta+S_%7Bdb%7D%2B%281-%5Cbeta%29db%5E2)\n\n![W:=W-\\alpha \\frac{dW}{\\sqrt{S_W}},\\ b:=b-\\alpha \\frac{db}{\\sqrt{S_b}}](https://www.zhihu.com/equation?tex=W%3A%3DW-%5Calpha+%5Cfrac%7BdW%7D%7B%5Csqrt%7BS_W%7D%7D%2C%5C+b%3A%3Db-%5Calpha+%5Cfrac%7Bdb%7D%7B%5Csqrt%7BS_b%7D%7D)\n\n下面简单解释一下RMSprop算法的原理，仍然以下图为例，为了便于分析，令水平方向为W的方向，垂直方向为b的方向。\n\n![](https://pic3.zhimg.com/80/v2-fbc62d5643928f56edf1e0b007a91d97_hd.jpg)\n\n从图中可以看出，梯度下降（蓝色折线）在垂直方向（b）上振荡较大，在水平方向（W）上振荡较小，表示在b方向上梯度较大，即 ![db](https://www.zhihu.com/equation?tex=db) 较大，而在W方向上梯度较小，即 ![dW](https://www.zhihu.com/equation?tex=dW) 较小。因此，上述表达式中 ![S_b](https://www.zhihu.com/equation?tex=S_b) 较大，而 ![S_W](https://www.zhihu.com/equation?tex=S_W) 较小。在更新W和b的表达式中，变化值 ![\\frac{dW}{\\sqrt{S_W}}](https://www.zhihu.com/equation?tex=%5Cfrac%7BdW%7D%7B%5Csqrt%7BS_W%7D%7D) 较大，而 ![\\frac{db}{\\sqrt{S_b}}](https://www.zhihu.com/equation?tex=%5Cfrac%7Bdb%7D%7B%5Csqrt%7BS_b%7D%7D) 较小。也就使得W变化得多一些，b变化得少一些。即加快了W方向的速度，减小了b方向的速度，减小振荡，实现快速梯度下降算法，其梯度下降过程如绿色折线所示。总得来说，就是如果哪个方向振荡大，就减小该方向的更新速度，从而减小振荡。\n\n还有一点需要注意的是为了避免RMSprop算法中分母为零，通常可以在分母增加一个极小的常数 ![\\varepsilon](https://www.zhihu.com/equation?tex=%5Cvarepsilon) ：\n\n![W:=W-\\alpha \\frac{dW}{\\sqrt{S_W}+\\varepsilon},\\ b:=b-\\alpha \\frac{db}{\\sqrt{S_b}+\\varepsilon}](https://www.zhihu.com/equation?tex=W%3A%3DW-%5Calpha+%5Cfrac%7BdW%7D%7B%5Csqrt%7BS_W%7D%2B%5Cvarepsilon%7D%2C%5C+b%3A%3Db-%5Calpha+%5Cfrac%7Bdb%7D%7B%5Csqrt%7BS_b%7D%2B%5Cvarepsilon%7D)\n\n其中， ![\\varepsilon=10^{-8}](https://www.zhihu.com/equation?tex=%5Cvarepsilon%3D10%5E%7B-8%7D) ，或者其它较小值。\n\n## **8\\. Adam optimization algorithm**\n\nAdam（Adaptive Moment Estimation）算法结合了动量梯度下降算法和RMSprop算法。其算法流程为：\n\n![V_{dW}=0,\\ S_{dW},\\ V_{db}=0,\\ S_{db}=0](https://www.zhihu.com/equation?tex=V_%7BdW%7D%3D0%2C%5C+S_%7BdW%7D%2C%5C+V_%7Bdb%7D%3D0%2C%5C+S_%7Bdb%7D%3D0)\n\n![On\\ iteration\\ t:](https://www.zhihu.com/equation?tex=On%5C+iteration%5C+t%3A)\n\n![\\ \\ \\ \\ Cimpute\\ dW,\\ db](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+Cimpute%5C+dW%2C%5C+db)\n\n![\\ \\ \\ \\ V_{dW}=\\beta_1V_{dW}+(1-\\beta_1)dW,\\ V_{db}=\\beta_1V_{db}+(1-\\beta_1)db](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+V_%7BdW%7D%3D%5Cbeta_1V_%7BdW%7D%2B%281-%5Cbeta_1%29dW%2C%5C+V_%7Bdb%7D%3D%5Cbeta_1V_%7Bdb%7D%2B%281-%5Cbeta_1%29db)\n\n![\\ \\ \\ \\ S_{dW}=\\beta_2S_{dW}+(1-\\beta_2)dW^2,\\ S_{db}=\\beta_2S_{db}+(1-\\beta_2)db^2](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+S_%7BdW%7D%3D%5Cbeta_2S_%7BdW%7D%2B%281-%5Cbeta_2%29dW%5E2%2C%5C+S_%7Bdb%7D%3D%5Cbeta_2S_%7Bdb%7D%2B%281-%5Cbeta_2%29db%5E2)\n\n![\\ \\ \\ \\ V_{dW}^{corrected}=\\frac{V_{dW}}{1-\\beta_1^t},\\ V_{db}^{corrected}=\\frac{V_{db}}{1-\\beta_1^t}](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+V_%7BdW%7D%5E%7Bcorrected%7D%3D%5Cfrac%7BV_%7BdW%7D%7D%7B1-%5Cbeta_1%5Et%7D%2C%5C+V_%7Bdb%7D%5E%7Bcorrected%7D%3D%5Cfrac%7BV_%7Bdb%7D%7D%7B1-%5Cbeta_1%5Et%7D)\n\n![\\ \\ \\ \\ S_{dW}^{corrected}=\\frac{S_{dW}}{1-\\beta_2^t},\\ S_{db}^{corrected}=\\frac{S_{db}}{1-\\beta_2^t}](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+S_%7BdW%7D%5E%7Bcorrected%7D%3D%5Cfrac%7BS_%7BdW%7D%7D%7B1-%5Cbeta_2%5Et%7D%2C%5C+S_%7Bdb%7D%5E%7Bcorrected%7D%3D%5Cfrac%7BS_%7Bdb%7D%7D%7B1-%5Cbeta_2%5Et%7D)\n\n![\\ \\ \\ \\ W:=W-\\alpha\\frac{V_{dW}^{corrected}}{\\sqrt{S_{dW}^{corrected}}+\\varepsilon},\\ b:=b-\\alpha\\frac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected}}+\\varepsilon}](https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+W%3A%3DW-%5Calpha%5Cfrac%7BV_%7BdW%7D%5E%7Bcorrected%7D%7D%7B%5Csqrt%7BS_%7BdW%7D%5E%7Bcorrected%7D%7D%2B%5Cvarepsilon%7D%2C%5C+b%3A%3Db-%5Calpha%5Cfrac%7BV_%7Bdb%7D%5E%7Bcorrected%7D%7D%7B%5Csqrt%7BS_%7Bdb%7D%5E%7Bcorrected%7D%7D%2B%5Cvarepsilon%7D)\n\nAdam算法包含了几个超参数，分别是： ![\\alpha,\\beta_1,\\beta_2,\\varepsilon](https://www.zhihu.com/equation?tex=%5Calpha%2C%5Cbeta_1%2C%5Cbeta_2%2C%5Cvarepsilon) 。其中， ![\\beta_1](https://www.zhihu.com/equation?tex=%5Cbeta_1) 通常设置为0.9， ![\\beta_2](https://www.zhihu.com/equation?tex=%5Cbeta_2) 通常设置为0.999， ![\\varepsilon](https://www.zhihu.com/equation?tex=%5Cvarepsilon) 通常设置为 ![10^{-8}](https://www.zhihu.com/equation?tex=10%5E%7B-8%7D) 。一般只需要对 ![\\beta_1](https://www.zhihu.com/equation?tex=%5Cbeta_1) 和 ![\\beta_2](https://www.zhihu.com/equation?tex=%5Cbeta_2) 进行调试。\n\n实际应用中，Adam算法结合了动量梯度下降和RMSprop各自的优点，使得神经网络训练速度大大提高。\n\n## **9\\. Learning rate decay**\n\n减小学习因子 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 也能有效提高神经网络训练速度，这种方法被称为learning rate decay。\n\nLearning rate decay就是随着迭代次数增加，学习因子 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 逐渐减小。下面用图示的方式来解释这样做的好处。下图中，蓝色折线表示使用恒定的学习因子 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) ，由于每次训练 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。绿色折线表示使用不断减小的 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) ，随着训练次数增加， ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。相比较恒定的 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 来说，learning rate decay更接近最优值。\n\n![](https://pic1.zhimg.com/80/v2-5e3c34d62e2508cc7e929507184ac644_hd.jpg)\n\nLearning rate decay中对 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 可由下列公式得到：\n\n![\\alpha=\\frac{1}{1+decay\\_rate*epoch}\\alpha_0](https://www.zhihu.com/equation?tex=%5Calpha%3D%5Cfrac%7B1%7D%7B1%2Bdecay%5C_rate%2Aepoch%7D%5Calpha_0)\n\n其中，deacy_rate是参数（可调），epoch是训练完所有样本的次数。随着epoch增加， ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 会不断变小。\n\n除了上面计算 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 的公式之外，还有其它可供选择的计算公式：\n\n![\\alpha=0.95^{epoch}\\cdot \\alpha_0](https://www.zhihu.com/equation?tex=%5Calpha%3D0.95%5E%7Bepoch%7D%5Ccdot+%5Calpha_0)\n\n![\\alpha=\\frac{k}{\\sqrt{epoch}}\\cdot \\alpha_0\\ \\ \\ \\ or\\ \\ \\ \\ \\frac{k}{\\sqrt{t}}\\cdot \\alpha_0](https://www.zhihu.com/equation?tex=%5Calpha%3D%5Cfrac%7Bk%7D%7B%5Csqrt%7Bepoch%7D%7D%5Ccdot+%5Calpha_0%5C+%5C+%5C+%5C+or%5C+%5C+%5C+%5C+%5Cfrac%7Bk%7D%7B%5Csqrt%7Bt%7D%7D%5Ccdot+%5Calpha_0)\n\n其中，k为可调参数，t为mini-bach number。\n\n除此之外，还可以设置 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 为关于t的离散值，随着t增加， ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 呈阶梯式减小。当然，也可以根据训练情况灵活调整当前的 ![\\alpha](https://www.zhihu.com/equation?tex=%5Calpha) 值，但会比较耗时间。\n\n## **10\\. The problem of local optima**\n\n在使用梯度下降算法不断减小cost function时，可能会得到局部最优解（local optima）而不是全局最优解（global optima）。之前我们对局部最优解的理解是形如碗状的凹槽，如下图左边所示。但是在神经网络中，local optima的概念发生了变化。准确地来说，大部分梯度为零的“最优点”并不是这些凹槽处，而是形如右边所示的马鞍状，称为saddle point。也就是说，梯度为零并不能保证都是convex（极小值），也有可能是concave（极大值）。特别是在神经网络中参数很多的情况下，所有参数梯度为零的点很可能都是右边所示的马鞍状的saddle point，而不是左边那样的local optimum。\n\n![](https://pic4.zhimg.com/80/v2-6d39e64779ab860636a23eb182a85993_hd.jpg)\n\n类似马鞍状的plateaus会降低神经网络学习速度。Plateaus是梯度接近于零的平缓区域，如下图所示。在plateaus上梯度很小，前进缓慢，到达saddle point需要很长时间。到达saddle point后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开saddle point，继续前进，只是在plateaus上花费了太多时间。\n\n![](https://pic4.zhimg.com/80/v2-f379a54c8025f9e016c0cfeed85d7a1f_hd.jpg)\n\n总的来说，关于local optima，有两点总结：\n\n*   **只要选择合理的强大的神经网络，一般不太可能陷入local optima**\n*   **Plateaus可能会使梯度下降变慢，降低学习速度**\n\n值得一提的是，上文介绍的动量梯度下降，RMSprop，Adam算法都能有效解决plateaus下降过慢的问题，大大提高神经网络的学习速度。\n\n**_我的CSDN博客地址：_**\n\n[Coursera吴恩达《优化深度神经网络》课程笔记（2）-- 优化算法](https://link.zhihu.com/?target=http%3A//blog.csdn.net/red_stone1/article/details/78348753)\n\n**更多AI资源请关注公众号：红色石头的机器学习之路（ID：redstonewill）**"]],[0,17],[22591,22591]]]]]]}